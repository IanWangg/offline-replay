{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import math\n",
    "import gzip\n",
    "import itertools\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "num_train = 60000  # 60k train examples\n",
    "num_test = 10000  # 10k test examples\n",
    "train_inputs_file_path = './MNIST_data/train-images-idx3-ubyte.gz'\n",
    "train_labels_file_path = './MNIST_data/train-labels-idx1-ubyte.gz'\n",
    "test_inputs_file_path = './MNIST_data/t10k-images-idx3-ubyte.gz'\n",
    "test_labels_file_path = './MNIST_data/t10k-labels-idx1-ubyte.gz'\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "\n",
    "class StandardScaler(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"Runs two ops, one for assigning the mean of the data to the internal mean, and\n",
    "        another for assigning the standard deviation of the data to the internal standard deviation.\n",
    "        This function must be called within a 'with <session>.as_default()' block.\n",
    "\n",
    "        Arguments:\n",
    "        data (np.ndarray): A numpy array containing the input\n",
    "\n",
    "        Returns: None.\n",
    "        \"\"\"\n",
    "        self.mu = np.mean(data, axis=0, keepdims=True)\n",
    "        self.std = np.std(data, axis=0, keepdims=True)\n",
    "        self.std[self.std < 1e-12] = 1.0\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"Transforms the input matrix data using the parameters of this scaler.\n",
    "\n",
    "        Arguments:\n",
    "        data (np.array): A numpy array containing the points to be transformed.\n",
    "\n",
    "        Returns: (np.array) The transformed dataset.\n",
    "        \"\"\"\n",
    "        return (data - self.mu) / self.std\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        \"\"\"Undoes the transformation performed by this scaler.\n",
    "\n",
    "        Arguments:\n",
    "        data (np.array): A numpy array containing the points to be transformed.\n",
    "\n",
    "        Returns: (np.array) The transformed dataset.\n",
    "        \"\"\"\n",
    "        return self.std * data + self.mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    def truncated_normal_init(t, mean=0.0, std=0.01):\n",
    "        torch.nn.init.normal_(t, mean=mean, std=std)\n",
    "        while True:\n",
    "            cond = torch.logical_or(t < mean - 2 * std, t > mean + 2 * std)\n",
    "            if not torch.sum(cond):\n",
    "                break\n",
    "            t = torch.where(cond, torch.nn.init.normal_(torch.ones(t.shape), mean=mean, std=std), t)\n",
    "        return t\n",
    "\n",
    "    if type(m) == nn.Linear or isinstance(m, EnsembleFC):\n",
    "        input_dim = m.in_features\n",
    "        truncated_normal_init(m.weight, std=1 / (2 * np.sqrt(input_dim)))\n",
    "        m.bias.data.fill_(0.0)\n",
    "\n",
    "\n",
    "class EnsembleFC(nn.Module):\n",
    "    __constants__ = ['in_features', 'out_features']\n",
    "    in_features: int\n",
    "    out_features: int\n",
    "    ensemble_size: int\n",
    "    weight: torch.Tensor\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, ensemble_size: int, weight_decay: float = 0., bias: bool = True) -> None:\n",
    "        super(EnsembleFC, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.ensemble_size = ensemble_size\n",
    "        self.weight = nn.Parameter(torch.Tensor(ensemble_size, in_features, out_features))\n",
    "        self.weight_decay = weight_decay\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(ensemble_size, out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        pass\n",
    "\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        w_times_x = torch.bmm(input, self.weight)\n",
    "        return torch.add(w_times_x, self.bias[:, None, :])  # w times x + b\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )\n",
    "\n",
    "\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, state_size, action_size, reward_size, ensemble_size, hidden_size=200, learning_rate=1e-3, use_decay=False):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nn1 = EnsembleFC(state_size + action_size, hidden_size, ensemble_size, weight_decay=0.000025)\n",
    "        self.nn2 = EnsembleFC(hidden_size, hidden_size, ensemble_size, weight_decay=0.00005)\n",
    "        self.nn3 = EnsembleFC(hidden_size, hidden_size, ensemble_size, weight_decay=0.000075)\n",
    "        self.nn4 = EnsembleFC(hidden_size, hidden_size, ensemble_size, weight_decay=0.000075)\n",
    "        self.use_decay = use_decay\n",
    "\n",
    "        self.output_dim = state_size + reward_size\n",
    "        # Add variance output\n",
    "        self.nn5 = EnsembleFC(hidden_size, self.output_dim * 2, ensemble_size, weight_decay=0.0001)\n",
    "\n",
    "        self.max_logvar = nn.Parameter((torch.ones((1, self.output_dim)).float() / 2).to(device), requires_grad=False)\n",
    "        self.min_logvar = nn.Parameter((-torch.ones((1, self.output_dim)).float() * 10).to(device), requires_grad=False)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.apply(init_weights)\n",
    "        self.swish = Swish()\n",
    "\n",
    "    def forward(self, x, ret_log_var=False):\n",
    "        nn1_output = self.swish(self.nn1(x))\n",
    "        nn2_output = self.swish(self.nn2(nn1_output))\n",
    "        nn3_output = self.swish(self.nn3(nn2_output))\n",
    "        nn4_output = self.swish(self.nn4(nn3_output))\n",
    "        nn5_output = self.nn5(nn4_output)\n",
    "\n",
    "        mean = nn5_output[:, :, :self.output_dim]\n",
    "\n",
    "        logvar = self.max_logvar - F.softplus(self.max_logvar - nn5_output[:, :, self.output_dim:])\n",
    "        logvar = self.min_logvar + F.softplus(logvar - self.min_logvar)\n",
    "\n",
    "        if ret_log_var:\n",
    "            return mean, logvar\n",
    "        else:\n",
    "            return mean, torch.exp(logvar)\n",
    "\n",
    "    def get_decay_loss(self):\n",
    "        decay_loss = 0.\n",
    "        for m in self.children():\n",
    "            if isinstance(m, EnsembleFC):\n",
    "                decay_loss += m.weight_decay * torch.sum(torch.square(m.weight)) / 2.\n",
    "                # print(m.weight.shape)\n",
    "                # print(m, decay_loss, m.weight_decay)\n",
    "        return decay_loss\n",
    "\n",
    "    def loss(self, mean, logvar, labels, inc_var_loss=True):\n",
    "        \"\"\"\n",
    "        mean, logvar: Ensemble_size x N x dim\n",
    "        labels: N x dim\n",
    "        \"\"\"\n",
    "        assert len(mean.shape) == len(logvar.shape) == len(labels.shape) == 3\n",
    "        inv_var = torch.exp(-logvar)\n",
    "        if inc_var_loss:\n",
    "            # Average over batch and dim, sum over ensembles.\n",
    "            mse_loss = torch.mean(torch.mean(torch.pow(mean - labels, 2) * inv_var, dim=-1), dim=-1)\n",
    "            var_loss = torch.mean(torch.mean(logvar, dim=-1), dim=-1)\n",
    "            total_loss = torch.sum(mse_loss) + torch.sum(var_loss)\n",
    "        else:\n",
    "            mse_loss = torch.mean(torch.pow(mean - labels, 2), dim=(1, 2))\n",
    "            total_loss = torch.sum(mse_loss)\n",
    "        return total_loss, mse_loss\n",
    "\n",
    "    def train(self, loss):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        loss += 0.01 * torch.sum(self.max_logvar) - 0.01 * torch.sum(self.min_logvar)\n",
    "        # print('loss:', loss.item())\n",
    "        if self.use_decay:\n",
    "            loss += self.get_decay_loss()\n",
    "        loss.backward()\n",
    "        # for name, param in self.named_parameters():\n",
    "        #     if param.requires_grad:\n",
    "        #         print(name, param.grad.shape, torch.mean(param.grad), param.grad.flatten()[:5])\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "offline",
   "language": "python",
   "name": "offline-rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
